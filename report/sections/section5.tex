\chapter{Testing}

This section contains all testings done on the programme output. To measure the rule accuracy, $k-fold cross-validation$ is chosen, in which both training and testing set are only used once for testing \cite{Kohavi95astudy}. In this project, a two-fold cross-validation estimator is applied ($k=1$). This means given a dataset, it is partitioned into two equal-sized sub-datasets. One is used for training (i.e being used as the programme input to get output rules) and another is used for testing (i.e being used for accuracy measurements for the output rules obtained earlier). The two are then swapped once, meaning the first half is for testing and the second half for training. Accuracy is taken as an average of that of both rounds.

While $holdout$ methods could have been used, $k-fold cross-validation$ estimator variance is lower than the other by averaging its estimation over $k$ different partitions \cite{Kohavi95astudy}, hence chosen for this project testing part.

In terms of performance, a $runtime$ measuring method is set at the beginning of the programme, which records starting time and returning finishing time to calculate the total runtime (measurement unit: milliseconds).

However, due to time constraints, there are only two datasets being used as testing sets for this programme: Iris (4 attributes, 150 tuples) and Wine datasets (13 attributes, 178 tuples) \cite{ucirepo}.


\section{Performance}

In terms of performance, on Iris dataset, average runtime of 5 runs is around $234ms$. On Wine dataset, it is to be $475ms$ which is doubled from the Iris dataset but explanable due to its attribute size difference.

However, performance can slow down significantly if $System.out.println()$ is used within the programme. It results in $8126ms$ ($\approx 8s$) for Iris dataset and $23212ms$ ($\approx 23s$) for Wine dataset.  

\section{Accuracy}

In terms of accuracy, as stated, a two-fold cross-validation technique is applied. The two dataset, Iris and Wine, is cross-validated twice: firstly with $\sigma_{min}$ = $\delta_{min}$ = $\gamma_{min}$ = 0.5 and secondly with $\sigma_{min}$ = $\delta_{min}$ = $\gamma_{min}$ = 0.7.

Figure 5.1 shows the result as:

\begin{figure}[h]
    \centering
    \includegraphics[width=5.5in, height=4.5in]{figures/test1}
    \caption[Accuracy measures obtained using two-fold cross-validation technique]{Accuracy measures obtained using two-fold cross-validation technique}
    \label{fig:figure5_1}
\end{figure}

From Figure 5.1, the programme shows a positive relationship between the number of rules and accuracy level, with both reaching $100\%$ accuracy at the end of the test. This suggests the programme is in the right direction, matching the common trend. There are fewer rules in Iris dataset compared to that of Wine, which is explanable due to the significant gap in attribute numbers between them. 
However, upon testing, despite rules constructed successfully, it was observed that the programme seemed to produce similar rules at most times despite changes in threshold requirements. Another test was then done setting all thresholds at a low $0.1$, where the programme was expected to produce much larger number of rules compared to the initial test. However, the programme freezed and a $Null$ error appeared in the console. This suggests a structural error in the recursive loop, although due to time constraint no tests could be done further.






