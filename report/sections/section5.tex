\chapter{Testing}

This section contains all testings done on the programme output. To measure the rule accuracy, $k-fold cross-validation$ is chosen, in which both training and testing set are only used once for testing \cite{Kohavi95astudy}. In this project, a two-fold cross-validation estimator is applied ($k=1$). This means given a dataset, it is partitioned into two equal-sized sub-datasets. One is used for training (i.e being used as the programme input to get output rules) and another is used for testing (i.e being used for accuracy measurements for the output rules obtained earlier). The two are then swapped once, meaning the first half is for testing and the second half for training. Accuracy is taken as an average of that of both rounds.

While $holdout$ methods could have been used, $k-fold cross-validation$ estimator variance is lower than the other by averaging its estimation over $k$ different partitions \cite{Kohavi95astudy}, hence chosen for this project testing part.

In terms of performance, a $runtime$ measuring method is set at the beginning of the programme, which records starting time and returning finishing time to calculate the total runtime (measurement unit: milliseconds).

However, due to time constraints, there are only two datasets being used as testing sets for this programme: Iris (4 attributes, 150 tuples and Wine datasets (13 attributes, 178 tuples) \cite{ucirepo}.


\section{Performance}

In terms of performance, on Iris dataset, average runtime of 5 runs is around $234ms$. On Wine dataset, it is to be $475ms$ which is doubled from the Iris dataset but explanable due to its attribute size difference.

However, performance can slow down significantly if $System.out.println()$ is used within the programme. It results in $8126ms$ ($\approx 8s$) for Iris dataset and $23212ms$ ($\approx 23s$) for Wine dataset.  

\section{Accuracy}

In terms of accuracy, from the project demonstration, despite giving output rules when tested, the programme shows semantically wrong output and crashes at certain threshold changes. It means the programme still produces classification rules at several threshold setups, but seemingly stays fixed around the same output even when threshold variation is significantly large. This indicates a structural error that is suspected to be caused by wrong parameters being passed into the recursive $Generator$ call, meaning the combining key process ($aPriori()$) is incorrectly recalled thus produces wrong combination at the end of loop.




